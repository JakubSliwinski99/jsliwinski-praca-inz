{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0780073836122166\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12028/671096221.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreacts\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlinkedin_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_index_prediction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\praca-inz-projekt\\scripts\\linkedin_model.py\u001B[0m in \u001B[0;36mget_index_prediction\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_index_prediction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mbest_post\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_best_post\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[0mdf_for_text\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_mini_model_for_a_text_linkedin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbest_post\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m     \u001B[1;31m# print(df_for_text)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\praca-inz-projekt\\scripts\\linkedin_model.py\u001B[0m in \u001B[0;36mget_mini_model_for_a_text_linkedin\u001B[1;34m(text, best_post)\u001B[0m\n\u001B[0;32m     53\u001B[0m     \u001B[0mnumber_of_ents\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0ments\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[0msimilarity_to_best\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdoc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msimilarity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbest_post_doc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m     \u001B[0mindex_higher_than_05\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_bayes_index_for_x_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m     \u001B[0mindex_higher_than_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_bayes_index_for_x_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[0mindex_higher_than_2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_bayes_index_for_x_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Desktop\\praca-inz-projekt\\scripts\\linkedin_model.py\u001B[0m in \u001B[0;36mget_bayes_index_for_x_index\u001B[1;34m(text, x_index)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m     \u001B[0mx_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtoarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     95\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMultinomialNB\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuba\\anaconda3\\envs\\aibd\\labagh\\lab3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mtransform\u001B[1;34m(self, raw_documents)\u001B[0m\n\u001B[0;32m   1377\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1378\u001B[0m         \u001B[1;31m# use the same matrix-building strategy as fit_transform\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1379\u001B[1;33m         \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_count_vocab\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mraw_documents\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfixed_vocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1380\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinary\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1381\u001B[0m             \u001B[0mX\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfill\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuba\\anaconda3\\envs\\aibd\\labagh\\lab3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1199\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mraw_documents\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1200\u001B[0m             \u001B[0mfeature_counter\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1201\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mfeature\u001B[0m \u001B[1;32min\u001B[0m \u001B[0manalyze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1202\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1203\u001B[0m                     \u001B[0mfeature_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvocabulary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mfeature\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuba\\anaconda3\\envs\\aibd\\labagh\\lab3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mdecoder\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdecoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0manalyzer\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m         \u001B[0mdoc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0manalyzer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\kuba\\anaconda3\\envs\\aibd\\labagh\\lab3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001B[0m in \u001B[0;36mdecode\u001B[1;34m(self, doc)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mdoc\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnan\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 226\u001B[1;33m             raise ValueError(\n\u001B[0m\u001B[0;32m    227\u001B[0m                 \u001B[1;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    228\u001B[0m             )\n",
      "\u001B[1;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from scripts import facebook_model, linkedin_model\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data_linkedin/linkedin_data.csv')\n",
    "text = df['POSTS'][233]\n",
    "reacts = df['INDEX'][233]\n",
    "\n",
    "print(reacts)\n",
    "\n",
    "result = linkedin_model.get_index_prediction(text)\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_facebook/facebook_data.csv')\n",
    "text = df['POSTS'][233]\n",
    "reacts = df['ALL_REACTIONS'][233]\n",
    "\n",
    "print(reacts)\n",
    "\n",
    "result = facebook_model.get_reactions_prediction(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.310167813549285\n",
      "38.99913375567249\n",
      "44.00179473308445\n",
      "36.632355703794936\n",
      "38.31551097318221\n",
      "36.066304864831736\n",
      "36.21149099244976\n",
      "37.18568327172651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_poisson_deviance\n",
    "from statsmodels.formula.api import glm\n",
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_of_expr = [\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY  + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ POLARITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\",\n",
    "    \"\"\"ALL_REACTIONS ~ SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "iter = 100\n",
    "\n",
    "for expr in list_of_expr:\n",
    "    deviance_sum = 0\n",
    "\n",
    "    for i in range(iter):\n",
    "        df_facebook = pd.read_csv('data_facebook/facebook_data.csv')\n",
    "\n",
    "        mask = np.random.rand(len(df_facebook)) < 0.85\n",
    "        df_train = df_facebook[mask]\n",
    "        df_test = df_facebook[~mask]\n",
    "        # print('Training data set length='+str(len(df_train)))\n",
    "        # print('Testing data set length='+str(len(df_test)))\n",
    "\n",
    "        # expr = \"\"\"ALL_REACTIONS ~ POLARITY + SUBJECTIVITY + POSTS_LENGTH + SIMILARITY_TO_BEST + TWENTY_REACTS_BAYES + FIFTY_REACTS_BAYES + HUNDRED_REACTS_BAYES + TWO_HUNDRED_REACTS_BAYES\"\"\"\n",
    "\n",
    "        y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')\n",
    "        y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')\n",
    "        # print(X_test)\n",
    "\n",
    "        poisson_training_results = sm.GLM(y_train, X_train, family=sm.families.Poisson()).fit()\n",
    "\n",
    "        poisson_predictions = poisson_training_results.get_prediction(X_test)\n",
    "        #summary_frame() returns a pandas DataFrame\n",
    "        predictions_summary_frame = poisson_predictions.summary_frame()\n",
    "        #print(predictions_summary_frame)\n",
    "\n",
    "        predicted_counts=predictions_summary_frame['mean']\n",
    "        actual_counts = y_test['ALL_REACTIONS']\n",
    "        # fig = plt.figure()\n",
    "        # fig.suptitle('Predicted versus actual reactions under a post')\n",
    "        # predicted, = plt.plot(X_test.index, predicted_counts, 'go-', label='Predicted counts')\n",
    "        # actual, = plt.plot(X_test.index, actual_counts, 'ro-', label='Actual counts')\n",
    "        # plt.legend(handles=[predicted, actual])\n",
    "        # plt.show()\n",
    "\n",
    "        #Print the training summary.\n",
    "        # print(poisson_training_results.summary())\n",
    "        # print(mean_poisson_deviance(actual_counts, predicted_counts))\n",
    "        deviance_sum += mean_poisson_deviance(actual_counts, predicted_counts)\n",
    "\n",
    "    print(deviance_sum / iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  POLARITY  SUBJECTIVITY  POSTS_LENGTH  SIMILARITY_TO_BEST  \\\n",
      "0        1.0       0.0           0.0             2            0.052449   \n",
      "\n",
      "   TWENTY_REACTS_BAYES  FIFTY_REACTS_BAYES  HUNDRED_REACTS_BAYES  \\\n",
      "0                    0                   0                     0   \n",
      "\n",
      "   TWO_HUNDRED_REACTS_BAYES  \n",
      "0                         0  \n"
     ]
    }
   ],
   "source": [
    "result = facebook_model.get_mini_model_for_a_text_facebook(\"Jestem kuba\", \"Najelpszy tekst\")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}